{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c3a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(dataset_fn, new_dataset_fn='0'):\n",
    "    if new_dataset_fn == '0':\n",
    "        new_dataset_fn = dataset_fn\n",
    "\n",
    "    df = pd.read_csv(dataset_fn, sep=\",\")\n",
    "    df = df.sample(frac=1)\n",
    "    df.to_csv(new_dataset_fn, sep=\",\", index=False)\n",
    "\n",
    "def combine_files(update_fn, train_fn, combined_fn):\n",
    "    update_entities_df = pd.read_csv(update_fn, sep=\",\")\n",
    "    train_entities_df = pd.read_csv(train_fn, sep=\",\")\n",
    "    combined_entities_df = pd.concat([train_entities_df, update_entities_df])\n",
    "    combined_entities_df.to_csv(combined_fn, index=False)\n",
    "    \n",
    "def subtract_files(complete_fn, subtract_fn, result_fn, entity_column=\"entity\"):\n",
    "    complete_df = pd.read_csv(complete_fn)\n",
    "    subtract_df = pd.read_csv(subtract_fn)\n",
    "    complete_df.drop(complete_df[complete_df[entity_column].isin(subtract_df[entity_column])].index, inplace = True)\n",
    "    complete_df.to_csv(result_fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81fbc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "am=False\n",
    "md=True\n",
    "dmg=False\n",
    "\n",
    "if am:\n",
    "    train_fn = \"data/AM_entities_units.csv\"\n",
    "    validation_fn = \"data/AM_entities_units_VALIDATION.csv\"\n",
    "    combined_fn = \"data/AM_entities_units_COMBINED.csv\"\n",
    "elif dmg:\n",
    "    train_fn = \"data/entities_dgm777k.csv\"\n",
    "    validation_fn = \"data/entities_dgm777k_VALIDATION.csv\"\n",
    "    combined_fn = \"data/entities_dmg777k_COMBINED.csv\"\n",
    "elif md:\n",
    "    train_fn = \"data/entities_md_raw_TRAIN.csv\"\n",
    "    validation_fn = \"data/entities_md_raw_VALIDATION.csv\"\n",
    "    combined_fn = \"data/entities_md_raw_COMBINED.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0760db",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_dataset(train)\n",
    "shuffle_dataset(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aadd243",
   "metadata": {},
   "source": [
    "Combine the train and validation file, so the combined file can be given to the embedding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe45c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_files(validation, train, combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253aef4",
   "metadata": {},
   "source": [
    "Once the embeddings have been created we split the file again, so the classifier can use only the training set for training, and validation set for validation. We keep this split intact in order to have the same entities that we can compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_w_emb_fn = combined[:-4]+\"_w_emb.csv\"\n",
    "\n",
    "validation_w_emb_fn = validation[:-4]+\"_w_emb.csv\"\n",
    "train_w_emb_fn = train[:-4]+\"_w_emb.csv\"\n",
    "\n",
    "subtract_files(combined_w_emb_fn, train, validation_w_emb_fn)\n",
    "subtract_files(combined_w_emb_fn, validation, train_w_emb_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
